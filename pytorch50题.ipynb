{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "params=list(net.parameters())\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "#打印某一层参数的形状\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1035, -0.1554,  0.0187, -0.1082, -0.0107, -0.1463, -0.0072,  0.0040,\n",
      "         -0.0151, -0.0173]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#随机输入一个向量，查看前向传播输出\n",
    "input = torch.randn(1,3,32,32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度初始化\n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度反向传播\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3808, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#损失函数\n",
    "target=torch.randn(1,10)\n",
    "criterion=nn.MSELoss()\n",
    "output=net(input)\n",
    "loss=criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#将梯度初始化，计算上一步loss的反向传播\n",
    "net.zero_grad()\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0122, -0.0081, -0.0046,  0.0195,  0.0332,  0.0025])\n"
     ]
    }
   ],
   "source": [
    "#计算43中loss的反向传播\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新权重 定义SGD优化器算法，学习率设置为0.01\n",
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用优化器更新权重\n",
    "optimizer.zero_grad()\n",
    "output=net(input)\n",
    "loss=criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()#更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造一个transform，将三通道(0,1)区间的数据转换成(-1,1)的数据\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Net()\n",
    "criterion2=nn.CrossEntropyLoss()\n",
    "optimizer2=optim.SGD(net2.parameters(),lr=0.001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练网络\n",
    "for epoch in range(2):\n",
    "    running_loss=0\n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        # 获取X,y对\n",
    "        inputs,labels=data\n",
    "        #初始化梯度\n",
    "        optimizer2.zero_grad()\n",
    "        #前馈\n",
    "        outputs=net2(inputs)\n",
    "        #计算损失\n",
    "        loss=criterion2(outputs,labels)\n",
    "        #计算梯度\n",
    "        loss.backward()\n",
    "        #更新权值\n",
    "        optimizer2.step()\n",
    "        #每2000个数据打印平均代价函数值\n",
    "        running_loss+=loss.item()\n",
    "        if i%2000==1999:\n",
    "            print('[%d, %5d]loss :%.3f'%(epoch+1,i+1,running_loss/2000))\n",
    "            runnung_loss=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用模型预测，取一些数据\n",
    "dataiter=iter(testloader)\n",
    "images,labels=dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"GroundTruth:\",\"\".join('%5s' % classes[labels[j]] for j in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.使用模型预测\n",
    "outputs = net2(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在测试集上打分\n",
    "correct=0\n",
    "total=0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images,labels=data\n",
    "        outputs=net(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=label.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存训练的模型\n",
    "PATH=\"./data.pth\"\n",
    "torch.save(net.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取保存的模型\n",
    "pretrained_net = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "net3 = Net()\n",
    "net3.load_state_dict(pretrained_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden=nn.Linear(784,256)\n",
    "        self.act=nn.ReLU()\n",
    "        self.output=nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a=self.act(self.hidden(x))\n",
    "        return self.output(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1310, -0.0021,  0.1003, -0.1797, -0.0330,  0.1736, -0.0189,  0.1641,\n",
       "         -0.0578, -0.3024],\n",
       "        [ 0.1065, -0.1049,  0.0303, -0.2346, -0.0266,  0.0580,  0.1156,  0.1001,\n",
       "         -0.1538, -0.2057]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(2,784)\n",
    "net=MLP()\n",
    "print(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#ModuleList 接收⼀个⼦模块的列表作为输⼊\n",
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "print(net[-1]) # 类似List的索引访问\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#ModuleDict 接收⼀个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作\n",
    "net=nn.ModuleDict({\n",
    "    \"linear\":nn.Linear(784,256),\n",
    "    \"act\":nn.ReLU()\n",
    "})\n",
    "net[\"output\"]=nn.Linear(256,10)\n",
    "print(net['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight torch.Size([256, 784]) <class 'torch.nn.parameter.Parameter'>\n",
      "linear.bias torch.Size([256]) <class 'torch.nn.parameter.Parameter'>\n",
      "output.weight torch.Size([10, 256]) <class 'torch.nn.parameter.Parameter'>\n",
      "output.bias torch.Size([10]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "#参数访问 用data访问数值 grad访问梯度\n",
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size(),type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#自定义层，Parameter 类其实是 Tensor 的⼦类， ParameterList 和 ParameterDict 分别定义参数的列表和字典\n",
    "class MyDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDense,self).__init__()\n",
    "        self.params=nn.ParameterList([nn.Parameter(torch.randn(4,4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i in range(len(self.params)):\n",
    "            x=torch.mm(x,self.params[i])\n",
    "        return x\n",
    "net=MyDense()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
